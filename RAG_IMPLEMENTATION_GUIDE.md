# RAG Pipeline Implementation Guide

## üéØ What is RAG?

**Retrieval Augmented Generation (RAG)** combines:
- **Retrieval**: Finding relevant information from your knowledge base
- **Augmentation**: Adding that information as context to the prompt
- **Generation**: LLM generates answer based on the context

**Why RAG?**
- LLMs have knowledge cutoff dates
- Your specific data isn't in the LLM's training
- RAG provides up-to-date, domain-specific answers
- Reduces hallucinations by grounding answers in facts

---

## üìä Architecture Flow

```
User Question
    ‚Üì
[1] Convert to Embedding (768-dim vector)
    ‚Üì
[2] Similarity Search in PostgreSQL (pgvector)
    ‚Üì
[3] Retrieve Top-K Similar Documents
    ‚Üì
[4] Build RAG Prompt (Context + Question)
    ‚Üì
[5] Send to Ollama LLM
    ‚Üì
[6] Return Answer to React Frontend
```

---

## üîß Implementation Details

### **1. Document Model** (`api/models.py`)

```python
class Document(models.Model):
    content = models.TextField()              # Original text
    embedding = VectorField(dimensions=768)   # Vector representation
    metadata = models.JSONField()             # Optional metadata
```

**Why each field?**
- `content`: Stores actual text to show LLM as context
- `embedding`: 768-dimensional vector for similarity search
- `metadata`: Track source, category, timestamp, etc.

**What are embeddings?**
- Numerical representation of text meaning
- Similar texts have similar vectors
- Enables mathematical similarity comparison
- Generated by embedding models (Ollama's gemma:1b)

---

### **2. Ollama Service** (`api/ollama_service.py`)

**Two main functions:**

#### `generate_embedding(text)`
- Converts text ‚Üí 768-dimensional vector
- Uses Ollama's embedding API
- Required for both ingestion and search

#### `generate_response(prompt)`
- Sends prompt to local LLM
- Replaces Gemini API
- Returns generated answer

**Why local Ollama?**
- No API costs
- Data privacy (everything local)
- No rate limits
- Works offline

---

### **3. Data Ingestion** (`backend/ingest_data.py`)

**When to run:**
- Initial setup (load knowledge base)
- Adding new documents
- Updating existing knowledge

**How it works:**
```python
for document in KNOWLEDGE_BASE:
    embedding = generate_embedding(document["content"])
    Document.objects.create(
        content=document["content"],
        embedding=embedding
    )
```

**Your data sources:**
- FAQs
- Documentation
- Product information
- Company policies
- Any text-based knowledge

---

### **4. Similarity Search** (`api/rag_service.py`)

**What is Cosine Similarity?**
- Measures angle between two vectors
- Range: -1 (opposite) to 1 (identical)
- Formula: `cos(Œ∏) = (A¬∑B) / (||A|| ||B||)`

**How it works:**
```sql
SELECT content, (embedding <=> query_vector) as distance
FROM documents
ORDER BY distance
LIMIT 3
```

**The `<=>` operator:**
- pgvector's cosine distance operator
- Lower distance = more similar
- Extremely fast (optimized for vectors)

**Why top-k (k=3)?**
- Balance between context and token limits
- More docs = better context but slower
- Fewer docs = faster but might miss info

---

### **5. RAG Prompt Construction**

**Structure:**
```
System Instruction
    ‚Üì
Retrieved Context (3 documents)
    ‚Üì
User Question
    ‚Üì
Constraints (answer only from context)
```

**Example:**
```
You are a helpful assistant. Answer based ONLY on the context.

Context:
Document 1: P Square uses RAG for intelligent responses...
Document 2: PostgreSQL stores vector embeddings...
Document 3: Ollama provides local LLM capabilities...

Question: How does P Square work?

Instructions:
- Answer ONLY from context above
- If not in context, say "I don't have that information"
- Be concise and accurate

Answer:
```

**Why this structure?**
- Clear instructions prevent hallucinations
- Context provides factual grounding
- Constraints ensure accuracy

---

### **6. Backend Integration** (`api/views.py`)

**ChatBotAPI now uses RAG:**
```python
def post(self, request):
    question = request.data.get("question")
    result = rag_query(question)  # RAG pipeline
    return Response({"answer": result})
```

**Flow:**
1. React sends POST to `/api/chat/`
2. Django receives question
3. RAG pipeline executes
4. Answer returned to React
5. React displays in chat UI

**Optional DatabaseQueryAPI:**
- Keeps old Gemini functionality
- For database queries (Titanic, state_data)
- Separate endpoint: `/api/database-query/`

---

## üöÄ Setup Instructions

### **1. Run Setup Script**
```bash
setup_rag.bat
```

This will:
- Create database migrations
- Enable pgvector extension
- Ingest sample data

### **2. Verify Ollama is Running**
```bash
ollama serve
```

### **3. Test Embedding Generation**
```bash
curl http://localhost:11434/api/embeddings -d '{
  "model": "gemma:1b",
  "prompt": "test"
}'
```

### **4. Start Application**
```bash
# Backend
cd backend
python manage.py runserver

# Frontend (new terminal)
cd frontend
npm start
```

---

## üìù Adding Your Own Data

### **Option 1: Edit ingest_data.py**
```python
KNOWLEDGE_BASE = [
    {
        "content": "Your FAQ answer here...",
        "metadata": {"category": "faq", "source": "website"}
    },
    # Add more...
]
```

### **Option 2: Create CSV Loader**
```python
import csv

with open('faqs.csv', 'r') as file:
    reader = csv.DictReader(file)
    for row in reader:
        embedding = generate_embedding(row['content'])
        Document.objects.create(
            content=row['content'],
            embedding=embedding,
            metadata={"source": "csv"}
        )
```

### **Option 3: API Endpoint for Dynamic Ingestion**
Create an admin endpoint to add documents via API.

---

## üîç How React Frontend Works

**No changes needed!** React continues to:
1. Send POST to `/api/chat/`
2. Receive JSON response: `{"answer": "..."}`
3. Display in chat interface

**What changed:**
- Backend intelligence (Gemini ‚Üí RAG + Ollama)
- Response quality (grounded in your data)
- No external API calls

**Frontend remains:**
```javascript
const response = await fetch('http://localhost:8000/api/chat/', {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json',
        'Authorization': `Token ${token}`
    },
    body: JSON.stringify({ question: userInput })
});
const data = await response.json();
// Display data.answer
```

---

## üß™ Testing the Pipeline

### **Test 1: Check Documents**
```python
python manage.py shell
>>> from api.models import Document
>>> Document.objects.count()
3  # Should show your ingested documents
```

### **Test 2: Test Similarity Search**
```python
>>> from api.rag_service import similarity_search
>>> results = similarity_search("How does RAG work?")
>>> for doc in results:
...     print(doc['content'][:50], doc['similarity_score'])
```

### **Test 3: Full RAG Query**
```python
>>> from api.rag_service import rag_query
>>> answer = rag_query("What is P Square?")
>>> print(answer)
```

### **Test 4: Via API**
```bash
curl -X POST http://localhost:8000/api/chat/ \
  -H "Authorization: Token YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is P Square?"}'
```

---

## üéì Key Concepts Explained

### **Embeddings**
- Text ‚Üí Numbers (vectors)
- Captures semantic meaning
- "king" - "man" + "woman" ‚âà "queen"
- Dimension: 768 (gemma:1b model)

### **Vector Database**
- PostgreSQL + pgvector extension
- Stores high-dimensional vectors
- Fast similarity search (HNSW index)
- Better than traditional keyword search

### **Cosine Similarity**
- Measures vector similarity
- Ignores magnitude, focuses on direction
- Perfect for text similarity
- Alternative: Euclidean distance, dot product

### **Top-K Retrieval**
- Retrieve K most similar documents
- K=3 is common (balance speed/context)
- Adjustable based on needs
- More K = more context but slower

---

## üîß Customization Options

### **Change Embedding Model**
```python
# In ollama_service.py
"model": "nomic-embed-text"  # Better embeddings
```

### **Change LLM Model**
```python
# In ollama_service.py
"model": "llama2"  # More capable LLM
```

### **Adjust Top-K**
```python
# In rag_service.py
similarity_search(query_text, top_k=5)  # More context
```

### **Add Metadata Filtering**
```python
# In rag_service.py
cursor.execute("""
    SELECT content FROM documents
    WHERE metadata->>'category' = %s
    ORDER BY embedding <=> %s::vector
    LIMIT %s
""", [category, query_embedding, top_k])
```

---

## üêõ Troubleshooting

### **Issue: "pgvector extension not found"**
```sql
-- In PostgreSQL
CREATE EXTENSION vector;
```

### **Issue: "Ollama connection refused"**
```bash
# Start Ollama
ollama serve

# Verify
curl http://localhost:11434/api/tags
```

### **Issue: "Embedding dimension mismatch"**
- Ensure all embeddings use same model
- Re-ingest data if model changed
- Check VectorField dimensions match

### **Issue: "No relevant documents found"**
- Check if documents are ingested
- Verify embedding generation works
- Try broader queries

---

## üìä Performance Tips

1. **Index vectors** for faster search:
```sql
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);
```

2. **Batch ingestion** for large datasets
3. **Cache embeddings** for common queries
4. **Use faster embedding models** (nomic-embed-text)
5. **Limit context length** to reduce LLM latency

---

## üéØ Next Steps

1. ‚úÖ Run `setup_rag.bat`
2. ‚úÖ Add your own documents to `KNOWLEDGE_BASE`
3. ‚úÖ Test via React frontend
4. ‚úÖ Monitor and improve responses
5. ‚úÖ Add more data sources (CSV, PDF, etc.)

---

## üìö Additional Resources

- **pgvector docs**: https://github.com/pgvector/pgvector
- **Ollama docs**: https://ollama.ai/docs
- **RAG concepts**: https://www.pinecone.io/learn/retrieval-augmented-generation/
- **Embeddings guide**: https://platform.openai.com/docs/guides/embeddings

---

**Your RAG pipeline is now complete!** üéâ

The system will answer questions based on your knowledge base using local Ollama, with no external API dependencies.

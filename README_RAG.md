# P Square Intelligent Chatbot - RAG Edition

An intelligent chatbot system with **RAG (Retrieval Augmented Generation)** capabilities that can answer questions from PDF documents using vector embeddings and local LLM.

## ğŸ¯ What This System Does

- **PDF Question Answering**: Ask questions about company_policy.pdf and get accurate answers
- **Vector Search**: Uses semantic similarity to find relevant information
- **Local AI**: Runs completely on your machine (Ollama + PostgreSQL)
- **Complete Privacy**: No external API calls, all data stays local

## ğŸ—ï¸ Architecture

```
React Frontend â†” Django Backend â†” Ollama (Embeddings + LLM) â†” PostgreSQL (pgvector)
```

## ğŸš€ Quick Start

### First Time Setup:

1. **Install Ollama Model**:
   ```bash
   ollama pull gemma:1b
   ```

2. **Setup System**:
   ```bash
   setup_rag_complete.bat
   ```

3. **Ingest PDF**:
   ```bash
   ingest_company_policy.bat
   ```

4. **Start Everything**:
   - Terminal 1: `ollama serve`
   - Terminal 2: `cd backend && python manage.py runserver`
   - Terminal 3: `cd frontend && npm start`

5. **Access**: http://localhost:3000

### Daily Usage:

```bash
# Start Ollama
ollama serve

# Start Backend
cd backend && python manage.py runserver

# Start Frontend
cd frontend && npm start
```

## ğŸ“ Sample Questions

**About Company Policy (RAG-powered):**
- "What is the vacation policy?"
- "How many sick days do I get?"
- "What are the working hours?"
- "Tell me about remote work policy"

## ğŸ¯ How It Works

1. **PDF Ingestion** (One-time):
   - Extract text from company_policy.pdf
   - Split into 500-char chunks
   - Generate 768-dim embeddings (Ollama)
   - Store in PostgreSQL with pgvector

2. **Question Answering** (Every query):
   - Convert question to embedding
   - Search similar document chunks (vector similarity)
   - Retrieve top 3 relevant chunks
   - Generate answer using Ollama LLM

## ğŸ“Š Technology Stack

- **Frontend**: React 19.2.3 with glassmorphism UI
- **Backend**: Django 4.2.7 + REST Framework
- **AI**: Ollama (gemma:1b) for embeddings + LLM
- **Database**: PostgreSQL with pgvector extension
- **Auth**: Token-based + Google OAuth

## ğŸ“ Project Structure

```
P Square/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ company_policy.pdf          # Your PDF document
â”‚   â”œâ”€â”€ ingest_pdf.py               # PDF â†’ Vector conversion
â”‚   â”œâ”€â”€ verify_rag_setup.py         # System verification
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ rag.py                  # RAG pipeline
â”‚       â”œâ”€â”€ ollama_service.py       # Ollama integration
â”‚       â””â”€â”€ views.py                # API endpoints
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ Components/
â”‚       â”‚   â””â”€â”€ Chatbot.js          # Chat interface
â”‚       â””â”€â”€ services/
â”‚           â””â”€â”€ authService.js      # API calls
â”‚
â”œâ”€â”€ ingest_company_policy.bat       # Load PDF into database
â”œâ”€â”€ setup_rag_complete.bat          # Complete setup
â””â”€â”€ Documentation/
    â”œâ”€â”€ SETUP_CHECKLIST.md          # Step-by-step checklist
    â”œâ”€â”€ RAG_SETUP_GUIDE.md          # Detailed guide
    â”œâ”€â”€ RAG_QUICK_START.md          # Quick reference
    â”œâ”€â”€ RAG_ARCHITECTURE.md         # Technical details
    â””â”€â”€ CHANGES_SUMMARY.md          # What was changed
```

## ğŸ“š Documentation

- **[SETUP_CHECKLIST.md](SETUP_CHECKLIST.md)** - Step-by-step setup checklist
- **[RAG_SETUP_GUIDE.md](RAG_SETUP_GUIDE.md)** - Complete setup guide with troubleshooting
- **[RAG_QUICK_START.md](RAG_QUICK_START.md)** - Quick reference for daily usage
- **[RAG_ARCHITECTURE.md](RAG_ARCHITECTURE.md)** - System architecture and data flow
- **[CHANGES_SUMMARY.md](CHANGES_SUMMARY.md)** - Summary of all changes made

## ğŸ”§ Configuration

### Environment Variables (.env):
```
DB_NAME=LLM
DB_USER=postgres
DB_PASSWORD=your_password
DB_HOST=localhost
DB_PORT=5432
GEMINI_API_KEY=your_key  # Optional, for SQL queries
```

## ğŸ§ª Testing

### Verify System:
```bash
cd backend
python verify_rag_setup.py
```

### Check Documents:
```bash
cd backend
python manage.py shell
>>> from api.models import Document
>>> Document.objects.count()
```

### Test RAG Query:
```bash
cd backend
python
>>> import os, django
>>> os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'backend.settings')
>>> django.setup()
>>> from api.rag import rag_query
>>> rag_query("What is the vacation policy?")
```

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| "Ollama not found" | Run: `ollama serve` |
| "No documents found" | Run: `ingest_company_policy.bat` |
| "PyPDF2 error" | Run: `pip install PyPDF2==3.0.1` |
| "Port in use" | Close existing processes |

## ğŸ” Security & Privacy

âœ… **Everything runs locally:**
- PDF processing: Local (PyPDF2)
- Embeddings: Local (Ollama)
- Vector storage: Local (PostgreSQL)
- LLM: Local (Ollama)
- No data sent to cloud
- Complete privacy

## ğŸ“ˆ Performance

- **PDF Ingestion**: ~30 seconds (one-time)
- **Query Response**: ~3-6 seconds
- **Vector Search**: ~50ms
- **LLM Generation**: ~2-5 seconds

## ğŸ“ Key Features

âœ… **RAG Pipeline**:
- PDF text extraction
- Intelligent chunking (500 chars, 50 overlap)
- Vector embeddings (768-dim)
- Semantic search (pgvector)
- Context-aware answers

âœ… **User Features**:
- Modern chat interface
- User authentication
- Chat history
- Real-time responses

âœ… **Technical Features**:
- Local AI (no external APIs)
- Vector similarity search
- Token-based auth
- CORS configured
- RESTful API

## ğŸ”„ Adding More PDFs

1. Place PDF in `backend/` folder
2. Edit `ingest_pdf.py` to include new PDF
3. Run: `python ingest_pdf.py`
4. Ask questions about the new content

## ğŸ¯ What Makes This Special

1. **Complete Privacy**: All AI processing happens locally
2. **No API Costs**: Uses free, local Ollama
3. **Semantic Search**: Finds answers by meaning, not keywords
4. **Easy to Extend**: Add more PDFs anytime
5. **Production Ready**: Full auth, chat history, modern UI

## ğŸ“ Quick Commands

```bash
# Setup (first time)
setup_rag_complete.bat

# Ingest PDF
ingest_company_policy.bat

# Verify
cd backend && python verify_rag_setup.py

# Start Ollama
ollama serve

# Start Backend
cd backend && python manage.py runserver

# Start Frontend
cd frontend && npm start
```

## ğŸ‰ You're Ready!

Follow the [SETUP_CHECKLIST.md](SETUP_CHECKLIST.md) for step-by-step instructions.

---

**Status**: âœ… RAG-Enabled v2.0  
**Last Updated**: 2024  
**Developer**: Sumit

# RAG System - Hybrid Mode (Gemini + Ollama)

## ğŸ¯ System Configuration

**Embeddings:** Gemini API (high quality, low cost)  
**LLM Answer:** Ollama (free, local, no API cost)

This reduces Gemini API usage by 90%!

## ğŸš€ Setup Steps

### 1. Install Ollama
```bash
# Download from https://ollama.ai
ollama pull gemma:1b
```

### 2. Ingest PDF (Uses Gemini for embeddings)
```bash
ingest_company_policy.bat
```

### 3. Start Servers
```bash
# Terminal 1: Ollama (for LLM answers)
ollama serve

# Terminal 2: Backend
cd backend
python manage.py runserver

# Terminal 3: Frontend
cd frontend
npm start
```

## ğŸ’° Cost Comparison

### Before (Full Gemini):
- Embedding: Gemini API âœ…
- LLM Answer: Gemini API âŒ (expensive)

### Now (Hybrid):
- Embedding: Gemini API âœ… (one-time per query)
- LLM Answer: Ollama âœ… (free, local)

**Result:** ~90% reduction in API costs!

## ğŸ“Š How It Works

```
User Question
    â†“
Gemini â†’ Generate embedding (API call)
    â†“
pgvector â†’ Search similar chunks (local)
    â†“
Retrieve top 3 chunks (local)
    â†“
Ollama â†’ Generate answer (local, free)
    â†“
Return answer
```

## âœ… Benefits

- âœ… High-quality embeddings (Gemini)
- âœ… Free LLM answers (Ollama)
- âœ… Fast responses
- âœ… Low API costs
- âœ… Privacy for answers

## ğŸ§ª Test

Ask: "What is the vacation policy?"

**Behind the scenes:**
1. Gemini creates embedding (1 API call)
2. pgvector finds similar chunks (local)
3. Ollama generates answer (free)

## ğŸ“ Summary

**Gemini:** Only for embeddings (cheap)  
**Ollama:** For all LLM answers (free)  
**Best of both worlds!**

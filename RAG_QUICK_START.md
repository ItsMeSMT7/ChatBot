# RAG System - Quick Reference

## ğŸš€ Quick Start (First Time)

```bash
1. setup_rag_complete.bat          # Install & verify
2. ingest_company_policy.bat       # Load PDF into database
3. start_all.bat                   # Start everything
```

## ğŸ“‹ Daily Usage

```bash
start_all.bat                      # Starts Ollama + Backend + Frontend
```

## ğŸ”§ System Components

| Component | Purpose | Port/Location |
|-----------|---------|---------------|
| **Ollama** | Embeddings + LLM | localhost:11434 |
| **Django Backend** | API Server | localhost:8000 |
| **React Frontend** | User Interface | localhost:3000 |
| **PostgreSQL** | Vector Database | localhost:5432 |

## ğŸ“Š Data Flow

```
User Question
    â†“
[Frontend] Send to /api/chat/
    â†“
[Backend] rag_query(question)
    â†“
[Ollama] Generate embedding
    â†“
[PostgreSQL] Search similar vectors (pgvector)
    â†“
[Backend] Get top 3 chunks
    â†“
[Ollama] Generate answer from context
    â†“
[Frontend] Display response
```

## ğŸ§ª Test Commands

### Check Ollama
```bash
curl http://localhost:11434/api/tags
```

### Check Documents
```bash
cd backend
python manage.py shell
>>> from api.models import Document
>>> Document.objects.count()
```

### Test RAG
```bash
cd backend
python verify_rag_setup.py
```

## ğŸ“ Sample Questions

**About Company Policy (RAG):**
- "What is the vacation policy?"
- "How many sick days do I get?"
- "What are the working hours?"
- "Tell me about remote work policy"

## ğŸ”„ Re-ingest PDF

If you update company_policy.pdf:
```bash
ingest_company_policy.bat
```

## ğŸ› Common Issues

| Problem | Solution |
|---------|----------|
| "Ollama not found" | Run: `ollama serve` |
| "No documents found" | Run: `ingest_company_policy.bat` |
| "PyPDF2 error" | Run: `pip install PyPDF2==3.0.1` |
| "Port in use" | Close other instances |

## ğŸ“ Key Files

```
backend/
â”œâ”€â”€ ingest_pdf.py              # PDF â†’ Vector conversion
â”œâ”€â”€ verify_rag_setup.py        # System verification
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ rag.py                 # RAG pipeline
â”‚   â”œâ”€â”€ ollama_service.py      # Ollama integration
â”‚   â””â”€â”€ views.py               # API endpoints
â””â”€â”€ company_policy.pdf         # Your PDF document

Root/
â”œâ”€â”€ ingest_company_policy.bat  # Load PDF
â”œâ”€â”€ setup_rag_complete.bat     # Full setup
â””â”€â”€ start_all.bat              # Start servers
```

## ğŸ¯ What's Happening Behind the Scenes

1. **PDF Ingestion**: 
   - Extracts text â†’ Chunks (500 chars) â†’ Embeddings (768-dim) â†’ PostgreSQL

2. **Query Processing**:
   - Question â†’ Embedding â†’ Similarity search â†’ Top 3 chunks â†’ LLM â†’ Answer

3. **Vector Search**:
   - Uses cosine similarity (<=> operator in pgvector)
   - Finds semantically similar content

## ğŸ’¡ Tips

- Keep Ollama running in background
- Chunk size: 500 chars (adjustable in ingest_pdf.py)
- Top K: 3 documents (adjustable in rag.py)
- Model: gemma:1b (fast, good quality)

## ğŸ” Privacy

âœ… Everything runs locally
âœ… No external API calls
âœ… Data stays on your machine
âœ… Ollama processes locally

---

**Need Help?** Check `RAG_SETUP_GUIDE.md` for detailed documentation.
